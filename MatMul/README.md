# Лабораторная работа 1 :octocat:
## Перемножение матриц
**Задача**: реализовать алгоритм перемножения матриц<br/>
**Язык**: Python<br/>
**Входные данные**: 2 матрицы размером от 100 х 100 до 2000 х 2000 каждая<br/>
**Выходные данные**: проверка корректности перемножения + время вычисления<br/>
## **Техническое обеспечение**
+ Процессор: Intel(R) Xeon(R) CPU @ 2.30GHz
+ Графический процессор: Tesla K80
+ Чип: Apple M1 (для сравнения с вычислениями, полученными с Google Colaboratory)
## **Описание**
Язык Python 3.7 с использованием библиотек Numba, Numpy.
В программе реализованы четыре функции: 
+ ```CPU_matmul_dot``` - функция перемножение матриц на CPU с помощью встроенной функции ```np.dot()``` в библиотеке Numpy.
+ ```CPU_matmul``` - функция перемножение матриц на CPU.
+ ```CPU_matmul_jit``` - функция перемножение матриц на CPU с использованием декоратора ```@jit```.
+ ```GPU_matmul_cuda_jit``` - функция перемножения матриц на GPU с применением декоратора ```@cuda.jit```. 
Декоратор ```cuda.jit``` предназначен в первую очередь для оформления функций-ядер, поскольку только тогда поддерживаются встроенные переменные вроде ```threadIdx``` и спецификаторы типа ```__shared__```. Поэтому, если необходимо написать аналог CUDA-ядра, надо использовать декорирование ```@cuda.jit```.
## **Результаты**
В таблице приведены результаты расчета случайно сгенерированных матриц на CPU и GPU соответственно:
Размер матрицы | Расчет на CPU (np.dot), сек | Расчет на CPU, сек | Расчет на CPU (@jit), сек | Расчет на GPU, сек | Ускорение (CPU/GPU)
------------ | ------------- | ------------- | ------------- | ------------- | ------------- |
50 | 0.004 | 0.113 | 0.467 | 0.573 | 0.197
128 | 0.003 | 1.429 | 0.003 | 0.005 | 282.802
 500 | 0.014 | 79.691 | 0.182 | 0.065 | 1233.712

 Также были получены результаты перемножения матриц на чипе M1 с целью сравнения вычислительных способностей с CPU и GPU, предоставленных Colaboratory:
 Размер матрицы | Расчет на CPU (np.dot), сек | Расчет на CPU (М1), сек |
 ------------ | ------------- | ------------- |
 50 | 0.0012 | 0.078 |
 128 | 0.0013 | 0.752 |
 500 | 0.0131 | 44.946 |
 <br/>

 ### График сравнения времени вычисления в зависимости от размера матриц (в Google Colab): <br/>

 ![малыезнач_колаб](https://drive.google.com/uc?export=view&id=1BvU07aNd11TAb37qHojbSg6Yc_pQGGg6)
 <br/>
 ### График сравнения времени вычисления на CPU (M1) в зависимости от размера матриц (в jupyter): <br/>

 ![малыезнач_М1](https://drive.google.com/uc?export=view&id=1RU5iBX4Q3RQ3A-q-4VqGSZG5RLGYwQ2d)
 <br/>
 По оси X - размер матрицы;<br/>
 По оси Y - время выполнения расчета перемножения матриц.<br/>

 Корректность перемножения матриц была осуществлена с помощью ```np.allclose()```.

 ## **Заключение**
 По результатам можно сделать вывод, что GPU действительно ускоряет процесс вычисления в разы, если сравнивать с вычислением на CPU. Также было замечено, что при малых размерах матриц скорости вычисления на CPU и GPU довольно одинаковы, а где-то CPU даже быстрее. <br/>
 Для интереса были также получены результаты вычисления матриц с помощью встроенной функции ```np.dot()``` и с помощью декоратора ```@jit``` . Можно заметить, что использование декоратора, позволяет существенно увеличить скорость вычислений. <br/>
 Ну и на десерт, были получены результаты перемножения матриц на чипе M1. Можно заметить, что скорость вычисления на чипе M1 примерно в раза быстрее, чем на CPU, предоставляемом Colabом, но этот результат по отношению с результатом вычислений на GPU, конечно же, оставляет желать лучшего.
